{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8Esf6kdhE+SWPX+KGaYlS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Word2Vec\n","\n","En este notebook, vamos a crear un modelo Word2Vec utilizando texto de un artículo de la wikipedia.\n","\n","Para ello, lo primero que vamos a hacer es instalar las dependencias necesarias. Necesitaremos `beautifulsoup` para hacer el *scrapping* del texto y `lxml` para parsear el código `html` que nos encontremos y poder extraer, por ejemplo, únicamente los párrafos."],"metadata":{"id":"646aYU8JiKHb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXeTMM7kiGaY"},"outputs":[],"source":["!pip install beautifulsoup4\n","!pip install lxml"]},{"cell_type":"markdown","source":["Ahora realizaremos los imports necesarios:"],"metadata":{"id":"8jPu-FLNj9iK"}},{"cell_type":"code","source":["import bs4 as bs\n","import urllib.request\n","import re\n","import nltk"],"metadata":{"id":"O84jqPOSidxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y seguidamente descargaremos los paquetes que `nltk` necesita para funcionar:"],"metadata":{"id":"BUBozWa7kjXZ"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"id":"3-7RXtnIig2v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ya tenemos el software necesario preparado. Ahora vamos a escoger una página de la Wikipedia para obtener todos los párrafos que se encuentren en ella y utilizarlos como nuestro *corpus*.\n","\n","Para ello, utilizaremos la librería `urllib` de Python:"],"metadata":{"id":"01s51rSpkqEK"}},{"cell_type":"code","source":["datos_wikipedia = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')"],"metadata":{"id":"vHtpg3MXintb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para evitar problemas, necesitaremos convertir el texto en una codificación UTF-8:"],"metadata":{"id":"YqKQX8PHk6i4"}},{"cell_type":"code","source":["articulo = datos_wikipedia.read().decode('utf-8')"],"metadata":{"id":"x0VbyZNWiv-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y ahora extraeremos todos los párrafos que existan en la página:"],"metadata":{"id":"5cnSsoCWlA7P"}},{"cell_type":"code","source":["parser = bs.BeautifulSoup(articulo, 'lxml')\n","parrafos = parser.find_all('p')"],"metadata":{"id":"0Ugsw1m3izxM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Seguidamente, los concatenaremos en un único string:"],"metadata":{"id":"LwavEPL0lHt2"}},{"cell_type":"code","source":["texto = \"\"\n","for p in parrafos:\n","    texto += p.text"],"metadata":{"id":"53J_u1Nri7Od"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto"],"metadata":{"id":"Ikiyau37jA10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pre-procesado\n","\n","Como ya sabemos, es muy importante limpiar el texto y eliminar las stop-words."],"metadata":{"id":"a6pw1ddqjDhB"}},{"cell_type":"markdown","source":["### Limpieza del texto\n","\n","Vamos a convertir el texto a minúsculas, luego a sustituir todos los caracteres que no sean letras por espacios, y por último a sustituir los espacios de forma que solo quede uno."],"metadata":{"id":"UB7QrZBdjHBg"}},{"cell_type":"code","source":["texto = texto.lower()\n","texto"],"metadata":{"id":"ucIcZW0NjIN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = re.sub(r'[^a-z]', ' ', texto)\n","texto"],"metadata":{"id":"eM0iQ7S7jKIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto = re.sub(r'\\s+', ' ', texto)\n","texto"],"metadata":{"id":"Of6jZqZDjQX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["palabras = nltk.word_tokenize(texto)\n","palabras"],"metadata":{"id":"sscKIDX6jtgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Número de palabras: {len(palabras)}')"],"metadata":{"id":"dJ-AFakkm478"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora que ya tenemos las palabras extraidas, podemos hacer un poco más de limpieza, eliminando las stop-words, como ya sabemos:"],"metadata":{"id":"f41MxV1XmpeF"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","palabras = [p for p in palabras if p not in stopwords.words('english')]\n","palabras"],"metadata":{"id":"p5BAEZbijw1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Número de palabras: {len(palabras)}')"],"metadata":{"id":"H24URUY-jE7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fijaos cómo hemos conseguido limpiar bastante el dataset, quitando practicamente 500 palabras que no aportan información (las stop-words).\n","\n","Sin embargo, si os fijáis, siguen habiendo cosas que no debería haber, como por ejemplo letras sueltas.\n","\n","Vamos a inspeccionar un poco:"],"metadata":{"id":"gEjFqSEQnAZM"}},{"cell_type":"code","source":["for p in palabras:\n","    if len(p) < 3:\n","        print(p)"],"metadata":{"id":"a3kPv7krnN56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como podemos observar, tenemos \"palabras\" que son solo las letras \"e\", \"g\" y \"r\", y además \"ai\". Lógicamente \"AI\" es una palabra y no deberíamos eliminarla (las siglas de Artificial Intelligence), pero las demás si podemos quitarlas:"],"metadata":{"id":"ZqWYdW9InVHK"}},{"cell_type":"code","source":["palabras = [p for p in palabras if p not in ['e', 'g', 'r']]\n","palabras"],"metadata":{"id":"SuFV7Fp_ng2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parece que ya no están presentes, pero vamos a asegurarnos:"],"metadata":{"id":"uTHozZRjnq1_"}},{"cell_type":"code","source":["for p in palabras:\n","    if len(p) < 3:\n","        print(p)"],"metadata":{"id":"CXLT-JMXnp4M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¡Perfecto! Pues ya podemos comenzar con el embedding Word2Vec. Para ello, podemos utilizar una librería llamada `gensim` que ya implementa este modelo y otros (más información en https://radimrehurek.com/gensim/).\n","\n","La importamos y creamos el objeto `Word2Vec` con las palabras que acabamos de limpiar.\n","\n","El parámetro `min_count` indica la frecuencia mínima que debe tener una palabra para que se incluya en el embedding. Esto quiere decir que si establecemos `min_count=2`, todas aquellas palabras que únicamente aparezcan una vez en nuestro texto, no se tendrán en cuenta en el embedding.\n","\n","Por otra parte, el primer argumento (`sentences`) debe ser una lista de oraciones. Nosotros, como las hemos juntado anteriormente (por facilitar el pre-procesamiento), tenemos solo una, así que tendremos que usar `[palabras]` como argumento. Si no, dará error, podéis comprobarlo :)\n","\n","¡Vamos al lio!"],"metadata":{"id":"lBFU8_VtnuZx"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","word2vec = Word2Vec([palabras], min_count=2)"],"metadata":{"id":"VprgoF2PjBdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora ya podemos comprobar qué tal funciona nuestro embedding. De acuerdo a lo que hemos visto en clase, deberia de ser capaz de encontrar palabras similares y distintas.\n","\n","Pero antes, veamos realmente qué es lo que ha sucedido.\n","\n","Por ejemplo, veamos cuál es la representación de la palabra \"machine\":"],"metadata":{"id":"GvZ66X42ofGx"}},{"cell_type":"code","source":["word2vec.wv['machine']"],"metadata":{"id":"3fzlRQrckhBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word2vec.wv['machine'].shape"],"metadata":{"id":"17j8nfWwo-xD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como podéis ver, es un vector de 100 dimensiones. Eso significa que hemos \"embutido\" (embebido, *embedded*) nuestras palabras en un espacio de 100 dimensiones.\n","\n","Veamos ahora algunas palabras similares:"],"metadata":{"id":"jOqPU_GCo_n9"}},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('machine')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"QNO010E3knJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No parece muy coherente...\n","\n","Hagamos un par de pruebas más para asegurarnos:"],"metadata":{"id":"tMku1fZ_pVsz"}},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('artificial')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"zEhetSc9k1Cl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('intelligence')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"avDLPB9MkurU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nada, definitivamente no parece funcionar muy bien.\n","\n","¿Qué está pasando?\n","\n","Pues que, por defecto, `Word2Vec` entrena el modelo subyacente (`CBOW` en nuestro caso, porque es el modelo por defecto y no hemos especificado lo contrario) durante 5 épocas. Esto puede ser poco, vamos a subirlo a 200.\n","\n","También podemos modificar otros parámetros interesantes, como:\n","\n","- `window`: el tamaño de la ventana de contexto. Por defecto es 5, vamos a subirlo a 7.\n","- `vector_size`: las dimensiones del embedding. Vamos a subirlas a 120.\n","- `sg`: 0 para modelo CBOW, 1 para modelo skip-gram.\n","\n","Vamos a ejecutar el modelo CBOW con una ventana de 7 y un embedding de 120 dimensiones, y a entrenarlo durante 200 épocas, a ver si mejora lo anterior:"],"metadata":{"id":"2qNASZUupg7h"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","word2vec = Word2Vec([palabras], min_count=2, window=7, vector_size=120, epochs=200)"],"metadata":{"id":"fGaX-kpLpwmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Veamos ahora qué tal el embedding:"],"metadata":{"id":"LcmfGQsWqxmW"}},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('machine')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"vMjFPK1uqvnx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parece un poco más coherente. Palabras como \"learning\", \"learn\", \"deep\", \"modeling\", \"sequence\", etc. tienen que ver con \"machine\".\n","\n","Veamos las otras:"],"metadata":{"id":"mHBvE9Nzqvny"}},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('artificial')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"KG2qTuB2qvnz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["palabras_similares = word2vec.wv.most_similar('intelligence')\n","for p in palabras_similares:\n","    print(p)"],"metadata":{"id":"rO4o20N8qvnz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nada mal, ¿no os parece?"],"metadata":{"id":"FUt0ePLErDzu"}},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Construid un modelo Word2Vec, usando el modelo **skip-gram**, para la página misma página de Wikipedia que hemos usado en el ejemplo.\n","\n","Comparad los resultados. ¿Qué opináis, funciona mejor o peor?"],"metadata":{"id":"RtoiihNkrIs0"}},{"cell_type":"code","source":[],"metadata":{"id":"VscKrecVl-g0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Tls626HlmtP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Recursos:\n","- https://www.kaggle.com/code/vipulgandhi/bag-of-words-model-for-beginners\n","- https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031"],"metadata":{"id":"WV_JVPYgPwm1"}},{"cell_type":"code","source":[],"metadata":{"id":"m3vmeBEOPyFr"},"execution_count":null,"outputs":[]}]}