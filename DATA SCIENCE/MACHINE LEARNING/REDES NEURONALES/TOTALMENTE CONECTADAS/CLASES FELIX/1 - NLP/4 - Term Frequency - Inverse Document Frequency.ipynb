{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWGwIz+2Mghxh/LoDWEUrA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Term Frequency - Inverse Document Frequency\n","\n","Aunque en este punto ya tenemos una buena representación vectorial de nuestros textos, sigue habiendo un problema: **la representación creada no está normalizada**. Esta no normalización plantea dos **problemas**:\n","\n","- A nivel de *documento* (las filas de nuestra matriz de datos) **cada texto** lleva una escala completamente libre y hace **imposible compararlos** entre sí. Un **texto más largo tendrá contadores con valores mayores que un texto más corto**. En un ejemplo llevado al extremo podemos comparar un tuit con la noticia que enlaza ese tuit. Ambos documentos versarán sobre el mismo tema, pero no podrán compararse debido a la volumetría de ambos.\n","\n","- A nivel de *palabras* es complicado **comparar cuáles son más relevantes y cuáles menos en un _corpus_ concreto**. Ya hemos eliminado las *palabras de parada*, pero, dependiendo del **bias** de nuestro *corpus* hay **palabras que no aportan demasiada información** y, por tanto, su incidencia en nuestro algoritmo de *machine learning* debería ser menor. Por ejemplo, imaginemos que tenemos un *corpus* de documentos que sólo hablan de los equipos de la Liga de Fútbol Profesional. En este corpus la palabra *\"fútbol \"* es completamente irrelevante, ya que todos los documentos hablan de ella. Por el contrario, palabras como *\"lesión \"* o *\"fichaje \"* son muy relevantes porque permiten subclasificar los documentos. Sin embargo, si nuestro *corpus* está formado por noticias de todo tipo, la palabra *'fútbol'* es muy relevante porque identifica un tipo de noticia.\n","\n","Para resolver este problema, se utiliza una normalización llamada **tf-idf** (*term-frecuency times inverse document-frecuency*). Se define mediante la siguiente ecuación\n","\n","$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n","\n","siendo $tf(t, d)$ el número de veces que el término (palabra) $t$ aparece en el documento $d$ y $idf(t)$ se define como:\n","\n","$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n","\n","donde $n$ es el número de documentos de nuestro *corpus* y $df(t)$ es el número de documentos en los que aparece el término $t$.\n","\n","Posteriormente, los vectores se normalizan a nivel de documento (el módulo del vector de cada documento es 1).\n","\n","Analizando estas ecuaciones *tf-idf* observamos que aquellas palabras que tengan menor frecuencia de aparición serán más relevantes que las que aparezcan en más documentos.\n","\n","Esta transformación se puede realizar utilizando el objeto [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n","\n","Para ver cómo funciona, vamos a construir un *corpus* con varios documentos:"],"metadata":{"id":"HMotsTkmEV3r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wm-vCGWIERZ5"},"outputs":[],"source":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T14:14:26.420752Z","start_time":"2020-03-02T14:14:26.416011Z"},"id":"V7_qyymMhBKn"},"source":["corpus = [\n","    \"Este es el primer documento...\",\n","    \"Este documento no es el primero, sino el segundo documento.\",\n","    \"Y éste es el tercer documento.\",\n","    \"¿Es éste el primero? No, es el cuarto documento.\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Eliminamos caracteres extraños:"],"metadata":{"id":"BKebSWAYHfE7"}},{"cell_type":"code","source":["import re\n","import string\n","\n","# añadimos algunos más que no están en string.punctuation, como las comillas y \n","# las aperturas de interrogación/exclamación\n","# si no los añadiésemos, no se eliminarían\n","chars = string.punctuation + '“”¡¿'\n","\n","re_punc = re.compile('[%s]' % re.escape(chars))\n","# eliminar la puntuación de cada palabra\n","corpus = [re_punc.sub('', texto) for texto in corpus]\n","print(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713434307,"user_tz":-120,"elapsed":325,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"1d3e0bb0-fabf-46de-9db2-00b14ce826b9","id":"Oa9Hqi8FCx6G"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Este es el primer documento', 'Este documento no es el primero sino el segundo documento', 'Y éste es el tercer documento', 'Es éste el primero No es el cuarto documento']\n"]}]},{"cell_type":"markdown","source":["Y acentos:"],"metadata":{"id":"R8XfSoMCCx6H"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713434690,"user_tz":-120,"elapsed":7,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"19d89a9a-c8e2-4df0-9773-878f76a2cdab","id":"FmlPUKfoCx6I"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Este es el primer documento', 'Este documento no es el primero sino el segundo documento', 'Y este es el tercer documento', 'Es este el primero No es el cuarto documento']\n"]}],"source":["letras_con_acentos = [\n","    'á', 'é', 'í', 'ó', 'ú'\n","]\n","letras_sin_acentos = [\n","    'a', 'e', 'i', 'o', 'u'\n","]\n","\n","def quita_acentos(texto) -> str:\n","    res = texto\n","    for lca, lsa in zip(letras_con_acentos, letras_sin_acentos):\n","        res = res.replace(lca, lsa)\n","    return res\n","\n","corpus = [quita_acentos(texto) for texto in corpus]\n","print(corpus)"]},{"cell_type":"markdown","source":["Convertimos el texto a minúsculas:"],"metadata":{"id":"Insh8G8DCx6J"}},{"cell_type":"code","source":["corpus = [texto.lower() for texto in corpus]\n","print(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713435621,"user_tz":-120,"elapsed":7,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"6f0f64f1-9a94-45b0-ebb0-5bbcb1dc5331","id":"1Gp5lOnvCx6J"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['este es el primer documento', 'este documento no es el primero sino el segundo documento', 'y este es el tercer documento', 'es este el primero no es el cuarto documento']\n"]}]},{"cell_type":"markdown","source":["Aplicamos la técnica Bag of Words indicándole las stop words:"],"metadata":{"id":"ObsYLa7RDbd7"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","nltk.download('stopwords')\n","stop_words = stopwords.words('spanish')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713435888,"user_tz":-120,"elapsed":17,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"5b46e70b-f7f1-411a-defe-6ddc70a582b1","id":"gr4bPoOcCx6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["count_vectorizer = CountVectorizer(stop_words=stop_words)\n","X = count_vectorizer.fit_transform(corpus)\n","print(X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mx7bXMI0H8hS","executionInfo":{"status":"ok","timestamp":1683713435888,"user_tz":-120,"elapsed":13,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"d9b33418-8ebc-4bab-d857-f5dba6e969f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 1 0 0 0 0]\n"," [0 2 0 1 1 1 0]\n"," [0 1 0 0 0 0 1]\n"," [1 1 0 1 0 0 0]]\n"]}]},{"cell_type":"code","source":["print(count_vectorizer.get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713435890,"user_tz":-120,"elapsed":10,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"100baf37-4f98-4056-f408-be904ca435fe","id":"uZt0XRG9Cx6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['cuarto' 'documento' 'primer' 'primero' 'segundo' 'sino' 'tercer']\n"]}]},{"cell_type":"markdown","source":["Hasta aquí nada nuevo en el horizonte. Veamos ahora como aplicar la transformación TF-IDF:"],"metadata":{"id":"APf2PrZmICey"}},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T14:14:41.456651Z","start_time":"2020-03-02T14:14:41.451749Z"},"id":"K5gY-SmgguQY"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T14:14:47.085506Z","start_time":"2020-03-02T14:14:47.075349Z"},"id":"_3ELNstaiUOO"},"source":["counts = X.toarray()\n","X_transformed = tfidf_transformer.fit_transform(counts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnsUWNcpidic"},"source":["Y analizamos el resultado:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T14:14:52.625002Z","start_time":"2020-03-02T14:14:52.619476Z"},"id":"F5SJFW4Bie5C","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683713436149,"user_tz":-120,"elapsed":9,"user":{"displayName":"Félix José Fuentes Hurtado","userId":"08557990006797564533"}},"outputId":"4c7865c7-f435-4a6b-d8fd-d1f4d7f28b03"},"source":["print(X_transformed.toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.46263733 0.88654763 0.         0.         0.\n","  0.        ]\n"," [0.         0.54178991 0.         0.40927504 0.51911349 0.51911349\n","  0.        ]\n"," [0.         0.46263733 0.         0.         0.         0.\n","  0.88654763]\n"," [0.72664149 0.37919167 0.         0.5728925  0.         0.\n","  0.        ]]\n"]}]},{"cell_type":"markdown","source":["Fijaos que ahora las ocurrencias están ponderadas de acuerdo a las ecuaciones que hemos visto antes:\n","\n","$\\textrm{tf-idf}(t, d) = tf(t, d) \\times idf(t)$\n","\n","siendo $tf(t, d)$ el número de veces que el término (palabra) $t$ aparece en el documento $d$ y $idf(t)$:\n","\n","$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n","\n","donde $n$ es el número de documentos de nuestro *corpus* y $df(t)$ es el número de documentos en los que aparece el término $t$.\n","\n","De esta forma, los vectores están normalizados a nivel de documento (el módulo del vector de cada documento es 1), y aquellas palabras que tengan menor frecuencia de aparición serán más relevantes que las que aparezcan en más documentos."],"metadata":{"id":"gQ0x_543Iewb"}},{"cell_type":"code","source":[],"metadata":{"id":"Fn9cIjSzIMKz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Realiza la limpieza del dataset, la eliminación de stop-words, la vectorización del texto (bag of words) y la TF-IDF del siguiente *corpus* de documentos:\n","\n","> \"Cuando se juega al Juego de Tronos, solo se puede ganar o morir.\" - Cersei Lannister\n","\n","> \"Por qué será que en cuanto un hombre construye un muro, su vecino inmediatamente quiere saber qué hay del otro lado.\" - Tyrion Lannister\n","\n","> \"¿Qué es el honor, comparado con el amor de una mujer? ¿Qué es el deber, comparado con el calor de un hijo recién nacido entre los brazos, o el recuerdo de la sonrisa de un hermano? Aire y palabras. Aire y palabras. Solo somos humanos, y los dioses nos hicieron para el amor. Es nuestra mayor gloria y nuestra peor tragedia.\" - Maestre Aemon, Juego de Tronos\n","\n","> \"El hombre que dicta la condena debe blandir la espada.\" - Eddard Stark\n","\n","> \"El poder reside donde los hombres creen que reside. Es un truco, una sombra en la pared. Y un hombre muy pequeño puede proyectar una sombra muy grande.\" - Lord Varys"],"metadata":{"id":"_rWIR9XACDvM"}},{"cell_type":"code","source":["corpus = [\n","    \"Cuando se juega al Juego de Tronos, solo se puede ganar o morir.\",\n","    \"Por qué será que en cuanto un hombre construye un muro, su vecino inmediatamente quiere saber qué hay del otro lado.\",\n","    \"¿Qué es el honor, comparado con el amor de una mujer? ¿Qué es el deber, comparado con el calor de un hijo recién nacido entre los brazos, o el recuerdo de la sonrisa de un hermano? Aire y palabras. Aire y palabras. Solo somos humanos, y los dioses nos hicieron para el amor. Es nuestra mayor gloria y nuestra peor tragedia.\",\n","    \"El hombre que dicta la condena debe blandir la espada.\",\n","    \"El poder reside donde los hombres creen que reside. Es un truco, una sombra en la pared. Y un hombre muy pequeño puede proyectar una sombra muy grande.\"\n","]"],"metadata":{"id":"uqug2lmKCDvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3LkunKfwLWsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dS_DdABHLqGb"},"execution_count":null,"outputs":[]}]}